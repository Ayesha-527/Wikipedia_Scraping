# -*- coding: utf-8 -*-
"""NLP_Project_Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15WB6Rgjq7j4GRTJ3rsP1yx7ElD3ExyNi
"""

!pip install wikipedia-api

import wikipediaapi
import pandas as pd
import re
from nltk.tokenize import word_tokenize

import re

def clean_text(text):
    # Remove Broken Links
    text = re.sub(r'\[([^]]*)\]\((https?://[^\s)]+)\)', '', text)

    # Remove special characters except letters and numbers
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def fetch_clean_wikipedia_content(topic):
    headers = {'User-Agent': 'YourBotName/1.0 (your-contact-info)'}
    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, headers=headers)
    page_py = wiki_wiki.page(topic)

    if not page_py.exists():
        return None

    content = clean_text(page_py.text)
    url = page_py.fullurl

    return content, url

import nltk
nltk.download('punkt')

#Information Security Topics
infosec_topics = [
    "Cryptography",
    "Network Security",
    "Cybersecurity",
    "Firewall",
    "Phishing",
    "Computer Virus",
    "Encryption",
    "Secure Coding",
    "Computer Security",
    "Authentication",
    "Authorization",
    "Identity Theft",
    "Digital Signature",
    "Cybercrime",
    "Penetration Testing",
    "Incident Management",
    "Information Security Management",
    "Biometrics",
    "Information Privacy",
    "Web Application Security",
    "Data Security",
    "Social Engineering",
    "Malware Analysis",
    "Intrusion Detection System",
    "Privacy"
]



data = {'Topic': [], 'Content': [], 'URL': [], 'Type': [], 'Vocabulary Size': [], 'Word Count': []}

for topic in infosec_topics:
    content_and_url = fetch_clean_wikipedia_content(topic)
    if content_and_url is not None:
        content, url = content_and_url

        tokens = word_tokenize(content.lower())
        tokens = [word for word in tokens if word.isalpha()]
        vocab_size = len(set(tokens))
        word_count = len(tokens)
        content_type = 'Short' if word_count <= 1000 else 'Long'

        data['Topic'].append(topic)
        data['Content'].append(content)
        data['URL'].append(url)
        data['Type'].append(content_type)
        data['Vocabulary Size'].append(vocab_size)
        data['Word Count'].append(word_count)
    else:
        print(f"Unable to fetch content for topic: {topic}")

df = pd.DataFrame(data)
        df.to_csv('infosec_corpus.csv', index=False)

import nltk
nltk.download('wordnet')

import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Load the previously generated infosec_corpus.csv
df_infosec = pd.read_csv('infosec_corpus.csv')

data_tokens = {'Topic': [], 'Stems': [], 'Lemmas': []}

# Initialize NLTK's stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Function for tokenization, stemming, and lemmatization
def process_content(content):
    # Tokenize the content
    tokens = word_tokenize(content.lower())
    tokens = [word for word in tokens if word.isalpha()]

    # Perform stemming and lemmatization
    stems = [stemmer.stem(token) for token in tokens]
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]

    return ' '.join(stems), ' '.join(lemmas)

# Process each topic's content
for index, row in df_infosec.iterrows():
    topic = row['Topic']
    content = row['Content']

    stems, lemmas = process_content(content)

    data_tokens['Topic'].append(topic)
    data_tokens['Stems'].append(stems)
    data_tokens['Lemmas'].append(lemmas)

# Create DataFrame for tokens
df_tokens = pd.DataFrame(data_tokens)

# Save DataFrame to tokens.csv
df_tokens.to_csv('tokens.csv', index=False)

import pandas as pd

# Read the previously created infosec_corpus.csv file
df_corpus = pd.read_csv('infosec_corpus.csv')

# Create a DataFrame to store unique words and count for each topic
df_vocabulary = pd.DataFrame(columns=['Topic', 'Unique Words', 'Unique Word List'])

# Iterate through each row (topic) in the corpus
for index, row in df_corpus.iterrows():
    content = row['Content']
    tokens = content.split()  # Split content into tokens
    unique_words = list(set(tokens))  # Get unique words

    # Append the topic, unique word count, and unique words list to the df_vocabulary DataFrame
    df_vocabulary = df_vocabulary.append({
        'Topic': row['Topic'],
        'Unique Words': len(unique_words),
        'Unique Word List': ', '.join(unique_words)
    }, ignore_index=True)

# Save DataFrame to vocabulary.csv
df_vocabulary.to_csv('vocabulary.csv', index=False)

pip install textstat

import pandas as pd
import textstat

# Read the infosec_corpus.csv file
df_corpus = pd.read_csv('infosec_corpus.csv')

# Create a DataFrame to store readability for each topic
df_readability = pd.DataFrame(columns=['Topic', 'Readability'])

# Iterate through each row (topic) in the corpus
for index, row in df_corpus.iterrows():
    content = row['Content']
    readability_score = textstat.flesch_kincaid_grade(content)

    # Check if the readability score is <= 8 (easy) or > 8 (difficult)
    readability = 'Easy' if readability_score <= 8 else 'Difficult'

    # Append the topic and its readability to the df_readability DataFrame
    df_readability = df_readability.append({'Topic': row['Topic'], 'Readability': readability}, ignore_index=True)

# Save DataFrame to readability.csv
df_readability.to_csv('readability.csv', index=False)

pip install nltk

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# Download NLTK resources (run once)
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Read the infosec_corpus.csv file
df_corpus = pd.read_csv('infosec_corpus.csv')

# Create a DataFrame to store POS tagged data for each topic
df_pos = pd.DataFrame(columns=['Topic', 'Data with POS Tags'])

# Iterate through each row (topic) in the corpus
for index, row in df_corpus.iterrows():
    content = row['Content']
    tokens = word_tokenize(content)

    # Perform POS tagging on the tokens
    pos_tags = pos_tag(tokens)

    # Join token and POS tag pairs as a string
    pos_tagged_text = ' '.join([f"{word}/{tag}" for word, tag in pos_tags])

    # Append the topic and its POS tagged data to the df_pos DataFrame
    df_pos = df_pos.append({'Topic': row['Topic'], 'Data with POS Tags': pos_tagged_text}, ignore_index=True)

# Save DataFrame to pos.csv
df_pos.to_csv('pos.csv', index=False)

import pandas as pd
import numpy as np

# Load the infosec_corpus.csv file
df = pd.read_csv('infosec_corpus.csv')

# Determine the midpoint of the DataFrame
midpoint = len(df) // 2

# Assign 'Simple' to the first half and 'Difficult' to the second half
df['Readability'] = ['Simple'] * midpoint + ['Difficult'] * (len(df) - midpoint)

# Save the modified DataFrame to a new CSV file
df.to_csv('corpus_with_readabilityLevel.csv', index=False)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Read the CSV file
df = pd.read_csv('corpus_with_readabilityLevel.csv')

tfidf_vectorizer = TfidfVectorizer(stop_words='english')
X = tfidf_vectorizer.fit_transform(df['Content'])
y = df['Readability']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

#Predict the readability of each topic based on their contents using a trained model,
# Read the  data
df = pd.read_csv('infosec_corpus.csv')

# Transform the 'Content' column using the same vectorizer instance
X_new = tfidf_vectorizer.transform(df['Content'])

# Predict using the trained model
y_new_pred = model.predict(X_new)

# Add the predictions to the new_data DataFrame and save it
df['Predicted_Readability'] = y_new_pred
df.to_csv('coming_data_with_predictions.csv', index=False)